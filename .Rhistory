comb
names(comb)
names(comb) = comb_art
names(comb)
names(comb) = sub('.txt', '', names(comb))
names(comb)
names(comb) = comb_art
names(comb)
mynames = comb_art %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
unlist
names(comb) = mynames
names(comb)
names(comb) = mynames
names(comb)
names(comb)[5]
names(comb) = comb_art
names(comb) = sub('.txt', '', names(comb))
names(comb)[5]
#Create a text mining corpus
corp_tr=Corpus(VectorSource(comb))
corp_ts_cp=corp_ts #copy of the corp_tr file
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(tolower)) #convert to lower case
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(removeNumbers)) #remove numbers
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(removePunctuation)) #remove punctuation
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(stripWhitespace)) #remove excess space
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(removeWords),stopwords("en")) #removing stopwords. Not exploring much on this, to avoid losing out on valuable information.
#Create a text mining corpus
corp_ts=Corpus(VectorSource(comb1))
names(comb) = mynames
#Create a text mining corpus
corp_tr=Corpus(VectorSource(comb))
#Pre-processing and tokenization using tm_map function:
corp_tr_cp=corp_tr #copy of the corp_tr file
corp_tr_cp = tm_map(corp_tr_cp, content_transformer(tolower)) #convert to lower case
corp_tr_cp = tm_map(corp_tr_cp, content_transformer(removeNumbers)) #remove numbers
corp_tr_cp = tm_map(corp_tr_cp, content_transformer(removePunctuation)) #remove punctuation
corp_tr_cp = tm_map(corp_tr_cp, content_transformer(stripWhitespace)) #remove excess space
corp_tr_cp = tm_map(corp_tr_cp, content_transformer(removeWords),stopwords("en")) #removing stopwords. Not exploring much on this, to avoid losing out on valuable information.
DTM_train = DocumentTermMatrix(corp_tr_cp)
#Removing sparse items
DTM_tr=removeSparseTerms(DTM_train,0.99)
tf_idf_mat = weightTfIdf(DTM_tr)
DTM_trr<-as.matrix(tf_idf_mat) #X
tf_idf_mat #3394 words, 2500 documents
#repeat for test
test=Sys.glob('../data/ReutersC50/C50test/*')
comb_art1=NULL
labels1=NULL
for (name in test)
{
author1=strsplit(name,split="/")[[1]][5]
article1=Sys.glob(paste0(name,'/*.txt'))
comb_art1=append(comb_art1,article1)
labels1=append(labels1,rep(author1,length(article1)))
}
comb1 = lapply(comb_art1, readerPlain)
names(comb1) = comb_art1
names(comb1) = sub('.txt', '', names(comb1))
#Create a text mining corpus
corp_ts=Corpus(VectorSource(comb1))
##### *2.b.Pre-processing and tokenization*
#Pre-processing and tokenization using tm_map function:
corp_ts_cp=corp_ts #copy of the corp_tr file
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(tolower)) #convert to lower case
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(removeNumbers)) #remove numbers
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(removePunctuation)) #remove punctuation
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(stripWhitespace)) #remove excess space
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(removeWords),stopwords("en")) #removing stopwords. Not exploring much on this, to avoid losing out on valuable information.
#Ensuring same number of variables in test and train by specifying column names from the train document term matrix
DTM_ts=DocumentTermMatrix(corp_ts_cp,list(dictionary=colnames(DTM_tr)))
tf_idf_mat_ts = weightTfIdf(DTM_ts)
DTM_tss<-as.matrix(tf_idf_mat_ts) #Matrix
tf_idf_mat_ts #3394 words, 2500 documents
DTM_trr_1<-DTM_trr[,which(colSums(DTM_trr) != 0)]
DTM_tss_1<-DTM_tss[,which(colSums(DTM_tss) != 0)]
#### *- To use only the intersecting columns*
#8312500 elements in both.
DTM_tss_1 = DTM_tss_1[,intersect(colnames(DTM_tss_1),colnames(DTM_trr_1))]
DTM_trr_1 = DTM_trr_1[,intersect(colnames(DTM_tss_1),colnames(DTM_trr_1))]
#### **3.b.Extracting principal components**
mod_pca = prcomp(DTM_trr_1,scale=TRUE,rank=30)#only went to rank=30 otherwise it breaks my laptop
pred_pca=predict(mod_pca,newdata = DTM_tss_1)
#Until PC724 - 74.5, almost 75% of variance explained. Hence stopping at 724 out of 2500 principal components
plot(mod_pca,type='line')
pred_pca
summary(pred_pca)
names(pred_pca)
#### *-The dataset hopefully contains only the relevant and informational features for classifying the documents to the author rightly*
tr_class = data.frame(mod_pca$x[,1:10])
tr_class['author']=labels
tr_load = mod_pca$rotation[,1:10]
ts_class_pre <- scale(DTM_tss_1) %*% tr_load
ts_class <- as.data.frame(ts_class_pre)
ts_class['author']=labels1
train.X = subset(tr_class, select = -c(author))
test.X = subset(ts_class,select=-c(author))
train.author=as.factor(tr_class$author)
test.author=as.factor(ts_class$author)
library(class)
set.seed(1)
knn_pred=knn(train.X,test.X,train.author,k=1)
temp_knn=as.data.frame(cbind(knn_pred,test.author))
temp_knn_flag<-ifelse(as.integer(knn_pred)==as.integer(test.author),1,0)
sum(temp_knn_flag)
sum(temp_knn_flag)*100/nrow(temp_knn) #802
train_raw = Sys.glob('../data/ReutersC50/C50train/*')
train_raw
#repeat for test
test_raw=Sys.glob('../data/ReutersC50/C50test/*')
testx = NULL
testy = NULL
library(caret)
library(dplyr)
library(tm)
library(tidyverse)
library(slam)
library(proxy)
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
trainx=NULL
trainy=NULL
train_raw = Sys.glob('../data/ReutersC50/C50train/*')
#for each author
for (a in train_raw){
author=strsplit(a,split="/")[[1]][5]
allDoc=Sys.glob(paste0(name,'/*.txt'))#get all docs
trainx=append(trainx,allDoc)
trainy=append(trainy,rep(author,length(allDoc)))#for each doc, have the author
}
#now train has all training docs
#clean file names
train = lapply(train_raw, readerPlain)
names(train) = train_raw
mynames = train_raw %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
unlist
names(train) = mynames
#Create a text mining corpus
trainCorp_r = Corpus(VectorSource(train))
#Pre-processing and tokenization using tm_map function:
trainCorp = trainCorp_r %>%
tm_map(content_transformer(tolower))  %>%             # make everything lowercase
tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
tm_map(content_transformer(stripWhitespace))          # remove excess white-space
trainCorp = tm_map(trainCorp, content_transformer(removeWords), stopwords("en"))
DTM_tr = DocumentTermMatrix(trainCorp)
DTM_tr = removeSparseTerms(DTM_tr, 0.95)
tfidf_tr = weightTfIdf(DTM_tr)
X_tr<-as.matrix(tfidf_tr) #X
#repeat for test
test_raw=Sys.glob('../data/ReutersC50/C50test/*')
testx = NULL
testy = NULL
for (a in test_raw){
author=strsplit(a, split="/")[[1]][5]
allDoc=Sys.glob(paste0(name, '/*.txt'))
testx=append(test_raw, allDoc)
testy=append(testy, rep(author, length(allDoc)))
}
test = lapply(test_raw, readerPlain)
names(test) = test_raw
mynames2 = test_raw %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
unlist
names(test) = mynames2
testCorp_r = Corpus(VectorSource(test))
#Pre-processing and tokenization using tm_map function:
testCorp = testCorp_r %>%
tm_map(content_transformer(tolower))  %>%             # make everything lowercase
tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
tm_map(content_transformer(stripWhitespace))          # remove excess white-space
testCorp = tm_map(testCorp, content_transformer(removeWords), stopwords("en"))
DTM_te = DocumentTermMatrix(testCorp)
DTM_te = DocumentTermMatrix(DTM_te,list(dictionary=colnames(DTM_tr)))
tfidf_te = weightTfIdf(DTM_te)
X_te = as.matrix(tfidf_te)
scrub_cols = which(colSums(X_tr) == 0)
X_tr = X_tr[,-scrub_cols]
scrub_cols2 = which(colSums(X_te) == 0)
X_te = X_te[,-scrub_cols2]
#now train has all training docs
#clean file names
train = lapply(train_raw, readerPlain)
rm(list=ls())
library(caret)
library(dplyr)
library(tm)
library(tidyverse)
library(slam)
library(proxy)
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
trainx=NULL
trainy=NULL
train_raw = Sys.glob('../data/ReutersC50/C50train/*')
#for each author
for (a in train_raw){
author=strsplit(a,split="/")[[1]][5]
allDoc=Sys.glob(paste0(name,'/*.txt'))#get all docs
trainx=append(trainx,allDoc)
trainy=append(trainy,rep(author,length(allDoc)))#for each doc, have the author
}
#for each author
for (a in train_raw){
author=strsplit(a,split="/")[[1]][5]
allDoc=Sys.glob(paste0(a,'/*.txt'))#get all docs
trainx=append(trainx,allDoc)
trainy=append(trainy,rep(author,length(allDoc)))#for each doc, have the author
}
#now train has all training docs
#clean file names
train = lapply(train_raw, readerPlain)
library(plyr)
library(dplyr)
#now train has all training docs
#clean file names
train = lapply(train_raw, readerPlain)
library(magrittr)
#now train has all training docs
#clean file names
train = lapply(train_raw, readerPlain)
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
#now train has all training docs
#clean file names
train = lapply(train_raw, readerPlain)
getwd()
#now train has all training docs
#clean file names
trains = lapply(train_raw, readerPlain)
library(tm)
library(magrittr)
library(slam)
library(proxy)
library(caret)
library(plyr)
library(dplyr)
library(ggplot2)
library('e1071')
#now train has all training docs
#clean file names
trains = lapply(trainx, readerPlain)
rm(list=ls())
library(caret)
library(dplyr)
library(tm)
library(tidyverse)
library(slam)
library(proxy)
# library(tm)
# library(magrittr)
# library(slam)
# library(proxy)
# library(caret)
# library(plyr)
# library(dplyr)
# library(ggplot2)
# library('e1071')
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
trainx=NULL
trainy=NULL
train_dir = Sys.glob('../data/ReutersC50/C50train/*')
#for each author
for (a in train_dir){
author=strsplit(a,split="/")[[1]][5]
allDoc=Sys.glob(paste0(a,'/*.txt'))#get all docs
trainx=append(trainx,allDoc)
trainy=append(trainy,rep(author,length(allDoc)))#for each doc, have the author
}
#now train has all training docs
#clean file names
trainDocs = lapply(trainx, readerPlain)
names(trainDocs) = trainx
mynames = trainx %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
unlist
names(trainDocs) = mynames
#Create a text mining corpus
trainCorp_r = Corpus(VectorSource(trainDocs))
#Pre-processing and tokenization using tm_map function:
trainCorp = trainCorp_r %>%
tm_map(content_transformer(tolower))  %>%             # make everything lowercase
tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
tm_map(content_transformer(stripWhitespace))          # remove excess white-space
trainCorp = tm_map(trainCorp, content_transformer(removeWords), stopwords("en"))
DTM_tr = DocumentTermMatrix(trainCorp)
DTM_tr = removeSparseTerms(DTM_tr, 0.95)
tfidf_tr = weightTfIdf(DTM_tr)
X_tr<-as.matrix(tfidf_tr) #X
rm(list=ls())
library(caret)
library(dplyr)
library(tm)
library(tidyverse)
library(slam)
library(proxy)
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
trainx=NULL
trainy=NULL
train_dir = Sys.glob('../data/ReutersC50/C50train/*')
#for each author
for (a in train_dir){
author=strsplit(a,split="/")[[1]][5]
allDoc=Sys.glob(paste0(name,'/*.txt'))#get all docs
trainx=append(trainx,allDoc)
trainy=append(trainy,rep(author,length(allDoc)))#for each doc, have the author
}
#for each author
for (a in train_dir){
author=strsplit(a,split="/")[[1]][5]
allDoc=Sys.glob(paste0(a,'/*.txt'))#get all docs
trainx=append(trainx,allDoc)
trainy=append(trainy,rep(author,length(allDoc)))#for each doc, have the author
}
#now train has all training docs
#clean file names
trainDoc = lapply(trainx, readerPlain)
names(trainDoc) = trainx
mynames = trainx %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
unlist
names(trainDoc) = mynames
#Create a text mining corpus
trainCorp_r = Corpus(VectorSource(trainDoc))
#Pre-processing and tokenization using tm_map function:
trainCorp = trainCorp_r %>%
tm_map(content_transformer(tolower))  %>%             # make everything lowercase
tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
tm_map(content_transformer(stripWhitespace))          # remove excess white-space
trainCorp = tm_map(trainCorp, content_transformer(removeWords), stopwords("en"))
DTM_tr = DocumentTermMatrix(trainCorp)
DTM_tr = removeSparseTerms(DTM_tr, 0.95)
tfidf_tr = weightTfIdf(DTM_tr)
X_tr<-as.matrix(tfidf_tr) #X
test_dir = Sys.glob('../data/ReutersC50/C50test/*')
testx = NULL
testy = NULL
for (a in test_dir){
author=strsplit(a, split="/")[[1]][5]
allDoc=Sys.glob(paste0(a, '/*.txt'))
testx=append(testx, allDoc)
testy=append(testy, rep(author, length(allDoc)))
}
testDoc = lapply(testx, readerPlain)
names(testDoc) = testx
mynames2 = testx %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
unlist
names(testDoc) = mynames2
testCorp_r = Corpus(VectorSource(testDoc))
#Pre-processing and tokenization using tm_map function:
testCorp = testCorp_r %>%
tm_map(content_transformer(tolower))  %>%             # make everything lowercase
tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
tm_map(content_transformer(stripWhitespace))          # remove excess white-space
testCorp = tm_map(testCorp, content_transformer(removeWords), stopwords("en"))
DTM_te = DocumentTermMatrix(testCorp)
DTM_te = DocumentTermMatrix(DTM_te,list(dictionary=colnames(DTM_tr)))
tfidf_te = weightTfIdf(DTM_te)
X_te = as.matrix(tfidf_te)
library(magrittr)
DTM_te = DocumentTermMatrix(DTM_te,list(dictionary=colnames(DTM_tr)))
library(plyr)
DTM_te = DocumentTermMatrix(DTM_te,list(dictionary=colnames(DTM_tr)))
library(dplyr)
DTM_te = DocumentTermMatrix(DTM_te,list(dictionary=colnames(DTM_tr)))
library('e1071')
DTM_te = DocumentTermMatrix(DTM_te,list(dictionary=colnames(DTM_tr)))
rm(list=ls9)
rm(list=ls())
library(tm)
library(slam)
library(proxy)
library(caret)
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
trainDir = Sys.glob('../data/ReutersC50/C50train/*')
trainx = NULL
trainy = NULL
for (a in trainDir)
{
author = substring(a,first=50)#first= ; ensure less than string length
allDoc = Sys.glob(paste0(a,'/*.txt'))
trainx=append(trainx,allDoc)
trainy=append(trainy,rep(author,length(allDoc)))
}
train = lapply(trainx, readerPlain)
names(train) = trainx
names(train) = sub('.txt', '', names(train))
trainCorp_r = Corpus(VectorSource(train))
trainCorp = trainCorp_r %>%
tm_map(content_transformer(tolower))  %>%             # make everything lowercase
tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
tm_map(content_transformer(stripWhitespace))          # remove excess white-space
trainCorp = tm_map(trainCorp, content_transformer(removeWords), stopwords("en"))
DTM_tr = DocumentTermMatrix(trainCorp)
DTM_tr = removeSparseTerms(DTM_tr, 0.95)
tfidf_tr = weightTfIdf(DTM_tr)
X_tr = as.matrix(tfidf_tr) #X
test=Sys.glob('../data/ReutersC50/C50test/*')
testx = NULL
testy = NULL
for (a in test){
author = substring(a,first=50)#first= ; ensure less than string length
allDoc = Sys.glob(paste0(a,'/*.txt'))
testx = append(testx,allDoc)
testy = append(testy ,rep(author, length(allDoc)))
}
#Cleaning the file names!!
test = lapply(testx, readerPlain)
names(test) = testx
names(test) = sub('.txt', '', names(test))
#Create a text mining corpus
testCorp_r = Corpus(VectorSource(test))
testCorp = testCorp_r %>%
tm_map(content_transformer(tolower))  %>%             # make everything lowercase
tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
tm_map(content_transformer(stripWhitespace))          # remove excess white-space
testCorp = tm_map(testCorp, content_transformer(removeWords), stopwords("en"))
#Ensuring same number of variables in test and train by specifying column names from the train document term matrix
DTM_te = DocumentTermMatrix(testCorp,list(dictionary=colnames(DTM_tr)))
tfidf_te = weightTfIdf(DTM_te)
rm(list=ls())
library(tm)
library(slam)
library(proxy)
library(caret)
readerPlain = function(fname){
readPlain(elem=list(content=readLines(fname)),
id=fname, language='en') }
trainDir = Sys.glob('../data/ReutersC50/C50train/*')
trainx = NULL
trainy = NULL
for (a in trainDir){
author = strsplit(a, split = "/")[[1]][5]
allDoc = Sys.glob(paste0(a,'/*.txt'))
trainx=append(trainx,allDoc)
trainy=append(trainy,rep(author,length(allDoc)))
}
train = lapply(trainx, readerPlain)
mynames = trainx %>%
{ strsplit(., '/', fixed = TRUE) } %>%
{ lapply(., tail, n = 2) } %>%
{ lapply(., paste0, collapse = '') } %>%
unlist
names(train) = mynames
trainCorp_r = Corpus(VectorSource(train))
trainCorp = trainCorp_r %>%
tm_map(content_transformer(tolower))  %>%             # make everything lowercase
tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
tm_map(content_transformer(stripWhitespace))          # remove excess white-space
trainCorp = tm_map(trainCorp, content_transformer(removeWords), stopwords("en"))
DTM_tr = DocumentTermMatrix(trainCorp)
DTM_tr = removeSparseTerms(DTM_tr, 0.95)
tfidf_tr = weightTfIdf(DTM_tr)
X_tr = as.matrix(tfidf_tr) #X
test=Sys.glob('../data/ReutersC50/C50test/*')
testx = NULL
testy = NULL
for (a in test){
author = strsplit(a, split = "/")[[1]][5]
allDoc = Sys.glob(paste0(a,'/*.txt'))
testx = append(testx,allDoc)
testy = append(testy ,rep(author, length(allDoc)))
}
#Cleaning the file names!!
test = lapply(testx, readerPlain)
mynames2 = testx %>%
{ strsplit(., '/', fixed = TRUE) } %>%
{ lapply(., tail, n = 2) } %>%
{ lapply(., paste0, collapse = '') } %>%
unlist
names(test) = mynames2
#Create a text mining corpus
testCorp_r = Corpus(VectorSource(test))
testCorp = testCorp_r %>%
tm_map(content_transformer(tolower))  %>%             # make everything lowercase
tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
tm_map(content_transformer(stripWhitespace))          # remove excess white-space
testCorp = tm_map(testCorp, content_transformer(removeWords), stopwords("en"))
#Ensuring same number of variables in test and train by specifying column names from the train document term matrix
DTM_te = DocumentTermMatrix(testCorp,list(dictionary=colnames(DTM_tr)))
